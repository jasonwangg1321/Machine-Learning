{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project\n",
    "\n",
    "Name: Jason Wang\n",
    "\n",
    "USC ID: 5363148736\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import keras\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  films adapted from comic books have had plenty...      1\n",
      "1  every now and then a movie comes along from a ...      1\n",
      "2  youve got mail works alot better than it deser...      1\n",
      "3    jaws  is a rare film that grabs your attenti...      1\n",
      "4  moviemaking is a lot like being the general ma...      1\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "pos_training = []\n",
    "neg_training = []\n",
    "pos_test = []\n",
    "neg_test = []\n",
    "file_check = []\n",
    "words = []\n",
    "\n",
    "pos_folder = os.listdir('pos')\n",
    "neg_folder = os.listdir('neg')\n",
    "\n",
    "for i, file in enumerate(pos_folder):\n",
    "    if i <700: #0-699 for training, csvs are already sorted\n",
    "        file_check.append(file)\n",
    "        with open (os.path.join('pos',file), 'r') as f:\n",
    "            text = f.read()\n",
    "            text = re.sub(r'\\d', '', text)  # removing puncutation https://stackoverflow.com/questions/18429143/strip-punctuation-with-regex-python\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            text = re.sub(r'[_]', '', text)\n",
    "            word = text.split()\n",
    "            words.extend(word)\n",
    "            pos_training.append((text,1)) # binary encoding for sentiments\n",
    "    elif i > 700:\n",
    "        with open (os.path.join('pos',file), 'r') as f:\n",
    "            text = f.read()\n",
    "            text = re.sub(r'\\d', '', text) \n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            text = re.sub(r'[_]', ' ', text)\n",
    "            word = text.split()\n",
    "            words.extend(word)\n",
    "            pos_test.append((text,1))\n",
    "\n",
    "for i, file in enumerate(neg_folder):\n",
    "    if i < 700:\n",
    "        with open (os.path.join('neg',file), 'r') as f:\n",
    "            text = f.read()\n",
    "            text = re.sub(r'\\d', '', text)  \n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            text = re.sub(r'[_]', ' ', text)\n",
    "            word = text.split()\n",
    "            words.extend(word)\n",
    "            neg_training.append((text,-1))\n",
    "    elif i > 700:\n",
    "        with open (os.path.join('neg',file), 'r') as f:\n",
    "            text = f.read()\n",
    "            text = re.sub(r'\\d', '', text)  \n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            text = re.sub(r'[_]', ' ', text)\n",
    "            word = text.split()\n",
    "            words.extend(word)\n",
    "            neg_test.append((text,-1))\n",
    "\n",
    "# print(file_check)\n",
    "full_dataset = pos_training+ neg_training + pos_test + neg_test\n",
    "\n",
    "pos_training_df = pd.DataFrame(pos_training, columns=['text', 'label'])\n",
    "neg_training_df = pd.DataFrame(neg_training, columns=['text', 'label'])\n",
    "pos_test_df = pd.DataFrame(pos_test, columns=['text', 'label'])\n",
    "neg_test_df = pd.DataFrame(neg_test, columns=['text', 'label'])\n",
    "\n",
    "full_training_df = pd.concat([pos_training_df,neg_training_df])\n",
    "full_test_df = pd.concat([pos_test_df,neg_test_df])\n",
    "\n",
    "full_data_df = pd.DataFrame(full_dataset, columns=['text', 'label'])\n",
    "\n",
    "print(full_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total words: 1286868\n",
      "Number of Unique words: 46727\n",
      "Average Review Length: 644.0780780780781\n",
      "SD Review Length: 14.567780002739045\n"
     ]
    }
   ],
   "source": [
    "average = len(words)/1998\n",
    "count = Counter(words) \n",
    "lengths = []\n",
    "i = 0\n",
    "for j,row in enumerate(full_data_df['text']):\n",
    "    texts = row.split()\n",
    "    length = len(texts)\n",
    "    lengths.append(length)\n",
    "    i += np.sqrt((length - average)**2)\n",
    "\n",
    "sd = np.sqrt(i/1998)\n",
    "\n",
    "print(f'Number of total words: {len(words)}') \n",
    "print(f'Number of Unique words: {len(count)}')\n",
    "print(f'Average Review Length: {average}')\n",
    "print(f'SD Review Length: {sd}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data = full_data_df, x = lengths)\n",
    "plt.title('Histogram of Review Lengths')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "all_data_tokenize = full_data_df['text'].tolist()\n",
    "training_tokenize = full_training_df['text'].tolist()\n",
    "test_tokenize = full_test_df['text'].tolist()\n",
    "\n",
    "#https://stackoverflow.com/questions/54891464/is-it-better-to-keras-fit-to-text-on-the-entire-x-data-or-just-the-train-data\n",
    "# read it is better to only fit on training data to prevent data leakage/ bias in model\n",
    "\n",
    "tokenizer.fit_on_texts(training_tokenize)\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(training_tokenize)\n",
    "test_seq = tokenizer.texts_to_sequences(test_tokenize)\n",
    "all_seq = tokenizer.texts_to_sequences(all_data_tokenize)\n",
    "\n",
    "print(len(train_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "734.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_lengths = []\n",
    "for seq in all_seq:\n",
    "    sequence_lengths.append(len(seq))\n",
    "\n",
    "L = np.percentile(sequence_lengths, 70)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     3,  1072,  1487],\n",
       "       [    0,     0,     0, ...,    10,     2,  1950],\n",
       "       [    0,     0,     0, ...,     1,   610,  7304],\n",
       "       ...,\n",
       "       [    0,     0,     0, ..., 14164,    30,  2421],\n",
       "       [    0,     0,     0, ...,   230,     7,  6634],\n",
       "       [    0,     0,     0, ...,     9,    25,   208]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/46298793/how-does-choosing-between-pre-and-post-zero-padding-of-sequences-impact-results did some research and it seems that pre padding leads to more accuracy / less noise\n",
    "padded_training_seq = pad_sequences(train_seq, maxlen = int(np.floor(L)), padding = 'pre', truncating= 'pre' )\n",
    "padded_training_df = pd.DataFrame(padded_training_seq)\n",
    "\n",
    "padded_test_seq = pad_sequences(test_seq, maxlen = int(np.floor(L)), padding = 'pre', truncating= 'pre' )\n",
    "padded_test_df = pd.DataFrame(padded_test_seq)\n",
    "\n",
    "padded_training_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 734, 32)\n",
      "(1400, 23488)\n"
     ]
    }
   ],
   "source": [
    "# Word Embeddings\n",
    "# https://keras.io/api/layers/core_layers/embedding/ below is taken from keras.io\n",
    "training_top_5000 = np.where(padded_training_seq > 5000, 0, padded_training_seq)\n",
    "\n",
    "embed = Embedding(5001, 32)\n",
    "flatten = Flatten()\n",
    "\n",
    "training_output = embed(training_top_5000)\n",
    "training_output_flat = flatten(training_output)\n",
    "\n",
    "training_output = training_output.numpy()\n",
    "torch_training_output = torch.tensor(training_output)\n",
    "\n",
    "training_output_flat = training_output_flat.numpy()\n",
    "torch_training_output_flat = torch.tensor(training_output_flat)\n",
    "\n",
    "print(training_output.shape)\n",
    "print(training_output_flat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (dense1): Linear(in_features=23488, out_features=50, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (dense2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (droupout2): Dropout(p=0.5, inplace=False)\n",
      "  (dense3): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (act3): ReLU()\n",
      "  (droupout3): Dropout(p=0.5, inplace=False)\n",
      "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (outact): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Train a MLP with three (dense) hidden layers each of which has 50 ReLUs\n",
    "# and one output layer with a single sigmoid neuron. Use a dropout rate of\n",
    "# 20% for the first layer and 50% for the other layers. Use ADAM optimizer\n",
    "# and binary cross entropy loss (which is equivalent to having a softmax in the\n",
    "# output). To avoid overfitting, just set the number of epochs as 2. Use a batch size of 10.\n",
    "\n",
    "#https://machinelearningmastery.com/building-multilayer-perceptron-models-in-pytorch/\n",
    "# used the above as reference to building this mlp\n",
    "\n",
    "mlp = nn.Sequential()\n",
    "mlp.add_module('dense1', nn.Linear(23488, 50))\n",
    "mlp.add_module('act1', nn.ReLU())\n",
    "mlp.add_module('dropout1', nn.Dropout(p=0.2))\n",
    "mlp.add_module('dense2', nn.Linear(50,50))\n",
    "mlp.add_module('act2', nn.ReLU())\n",
    "mlp.add_module('droupout2', nn.Dropout(p=0.5))\n",
    "mlp.add_module('dense3', nn.Linear(50,50))\n",
    "mlp.add_module('act3', nn.ReLU())\n",
    "mlp.add_module('droupout3', nn.Dropout(p=0.5))\n",
    "mlp.add_module(\"output\", nn.Linear(50, 1))\n",
    "mlp.add_module(\"outact\", nn.Sigmoid())\n",
    "\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss is: 0.677640974521637\n",
      "Trainining Accuracy: 0.4857142857142857\n",
      "Epoch 2 Loss is: 0.5845378637313843\n",
      "Trainining Accuracy: 0.5578571428571428\n"
     ]
    }
   ],
   "source": [
    "training_labels = full_training_df['label']\n",
    "training_labels = training_labels.to_numpy()\n",
    "training_labels = torch.tensor(training_labels)\n",
    "training_labels = (training_labels+1)//2\n",
    "\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "training_tensor_dataset = TensorDataset(torch_training_output_flat, training_labels)\n",
    "train_dataloader = DataLoader(training_tensor_dataset, batch_size=10, shuffle = True)\n",
    "\n",
    "loss_type = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters())\n",
    "\n",
    "for i, epoch in enumerate(range(2)):\n",
    "    mlp.train()\n",
    "    training_correct = 0\n",
    "    training_total = 0\n",
    "    \n",
    "    # This part below was helped written with chatgpt, i was getting errors with the labels and how to corrrectly use dataloader with training the model.\n",
    "\n",
    "    for inputs, targets in train_dataloader:\n",
    "        #forward prop\n",
    "        outputs = mlp(inputs)\n",
    "        targets = targets.float()\n",
    "        loss = loss_type(outputs.squeeze(), targets)\n",
    "        #backward prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #grad descent\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss = loss.item()\n",
    "        predict = (outputs.squeeze() > 0.5)\n",
    "        training_correct += (predict== targets).sum().item()\n",
    "        training_total += targets.size(0)\n",
    "\n",
    "    print(f'Epoch {i+1} Loss is: {training_loss}')\n",
    "    training_accuracy = training_correct / training_total\n",
    "    print(f'Trainining Accuracy: {training_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5418060200668896\n"
     ]
    }
   ],
   "source": [
    "testing_top_5000 = np.where(padded_test_seq > 5000, 0, padded_test_seq)\n",
    "testing_output = embed(testing_top_5000)\n",
    "testing_output_flat = flatten(testing_output)\n",
    "\n",
    "testing_output = testing_output.numpy()\n",
    "torch_testing_output = torch.tensor(testing_output)\n",
    "\n",
    "testing_output_flat = testing_output_flat.numpy()\n",
    "torch_testing_output_flat = torch.tensor(testing_output_flat)\n",
    "\n",
    "testing_labels = full_test_df['label']\n",
    "testing_labels = testing_labels.to_numpy()\n",
    "testing_labels = torch.tensor(testing_labels)\n",
    "testing_labels = (testing_labels+1)//2\n",
    "\n",
    "testing_tensor_dataset = TensorDataset(torch_testing_output_flat, testing_labels)\n",
    "test_dataloader = DataLoader(testing_tensor_dataset, batch_size=10)\n",
    "\n",
    "mlp.eval()\n",
    "# https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch\n",
    "# disabling gradient calculation is useful for inference, reduces memory consumption for computations\n",
    "with torch.no_grad():\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        outputs = mlp(inputs)\n",
    "        predict = (outputs.squeeze() > 0.5)\n",
    "        test_correct += (predict== targets).sum().item()\n",
    "        test_total += targets.size(0)\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Dimensional CNN\n",
    "# After the embedding layer, insert a Conv1D layer. This convolutional layer\n",
    "# has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded\n",
    "# word representations 3 vector elements of the word embedding at a time. The\n",
    "# convolutional layer is followed by a 1D max pooling layer with a length and\n",
    "# stride of 2 that halves the size of the feature maps from the convolutional\n",
    "# layer. The rest of the network is the same as the neural network above\n",
    "\n",
    "training_top_5000_cnn = np.where(padded_training_seq > 5000, 0, padded_training_seq)\n",
    "\n",
    "embed = Embedding(5001, 32)\n",
    "C1D = Conv1D(32, kernel_size = 3)\n",
    "MaxPooling = MaxPooling1D(pool_size =2, strides = 2)\n",
    "flatten = Flatten()\n",
    "\n",
    "training_output_cnn = embed(training_top_5000_cnn)\n",
    "training_output_C1D = C1D(training_output_cnn)\n",
    "training_output_Maxpool = MaxPooling(training_output_C1D)\n",
    "training_output_cnn_flat = flatten(training_output_Maxpool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (dense1): Linear(in_features=11712, out_features=50, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (dense2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (droupout2): Dropout(p=0.5, inplace=False)\n",
      "  (dense3): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (act3): ReLU()\n",
      "  (droupout3): Dropout(p=0.5, inplace=False)\n",
      "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (outact): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = nn.Sequential()\n",
    "cnn.add_module('dense1', nn.Linear(11712, 50))\n",
    "cnn.add_module('act1', nn.ReLU())\n",
    "cnn.add_module('dropout1', nn.Dropout(p=0.2))\n",
    "cnn.add_module('dense2', nn.Linear(50,50))\n",
    "cnn.add_module('act2', nn.ReLU())\n",
    "cnn.add_module('droupout2', nn.Dropout(p=0.5))\n",
    "cnn.add_module('dense3', nn.Linear(50,50))\n",
    "cnn.add_module('act3', nn.ReLU())\n",
    "cnn.add_module('droupout3', nn.Dropout(p=0.5))\n",
    "cnn.add_module(\"output\", nn.Linear(50, 1))\n",
    "cnn.add_module(\"outact\", nn.Sigmoid())\n",
    "\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss is: 0.7236382961273193\n",
      "CNN Trainining Accuracy: 0.5135714285714286\n",
      "Epoch 2 Loss is: 0.7028557062149048\n",
      "CNN Trainining Accuracy: 0.525\n"
     ]
    }
   ],
   "source": [
    "training_output_cnn_flat = training_output_cnn_flat.numpy()\n",
    "cnn_training_tensor = torch.tensor(training_output_cnn_flat)\n",
    "\n",
    "cnn_training_tensor_dataset = TensorDataset(cnn_training_tensor, training_labels)\n",
    "cnn_train_dataloader = DataLoader(cnn_training_tensor_dataset, batch_size=10, shuffle = True)\n",
    "\n",
    "loss_type = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters())\n",
    "\n",
    "for i, epoch in enumerate(range(2)):\n",
    "    cnn.train()\n",
    "    cnn_training_correct = 0\n",
    "    cnn_training_total = 0\n",
    "    \n",
    "    for inputs, targets in cnn_train_dataloader:\n",
    "        #forward prop\n",
    "        cnn_outputs = cnn(inputs)\n",
    "        cnn_targets = targets.float()\n",
    "        cnn_loss = loss_type(cnn_outputs.squeeze(), cnn_targets)\n",
    "        #backward prop\n",
    "        optimizer.zero_grad()\n",
    "        cnn_loss.backward()\n",
    "        #grad descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        cnn_predict = (cnn_outputs.squeeze() > 0.5)\n",
    "        cnn_training_correct += (cnn_predict== targets).sum().item()\n",
    "        cnn_training_total += targets.size(0)\n",
    "\n",
    "    print(f'Epoch {i+1} Loss is: {cnn_loss.item()}')\n",
    "    cnn_training_accuracy = cnn_training_correct / cnn_training_total\n",
    "    print(f'CNN Trainining Accuracy: {cnn_training_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Test Accuracy: 0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "testing_top_5000_cnn = np.where(padded_test_seq > 5000, 0, padded_test_seq)\n",
    "\n",
    "testing_output_cnn = embed(testing_top_5000_cnn)\n",
    "testing_output_C1D = C1D(testing_output_cnn)\n",
    "testing_output_Maxpool = MaxPooling(testing_output_C1D)\n",
    "testing_output_cnn_flat = flatten(testing_output_Maxpool)\n",
    "\n",
    "testing_output_cnn_flat = testing_output_cnn_flat.numpy()\n",
    "cnn_testing_tensor = torch.tensor(testing_output_cnn_flat)\n",
    "\n",
    "cnn_testing_tensor_dataset = TensorDataset(cnn_testing_tensor, testing_labels)\n",
    "cnn_test_dataloader = DataLoader(cnn_testing_tensor_dataset, batch_size=10)\n",
    "\n",
    "cnn.eval()\n",
    "with torch.no_grad():\n",
    "    cnn_test_correct = 0\n",
    "    cnn_test_total = 0\n",
    "    for inputs, targets in cnn_test_dataloader:\n",
    "        cnn_outputs = cnn(inputs)\n",
    "        cnn_predict = (cnn_outputs.squeeze() > 0.5)\n",
    "        cnn_test_correct += (cnn_predict== targets).sum().item()\n",
    "        cnn_test_total += targets.size(0)\n",
    "\n",
    "cnn_test_accuracy = cnn_test_correct / cnn_test_total\n",
    "print(f'CNN Test Accuracy: {cnn_test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 307ms/step - accuracy: 0.4879 - loss: 0.6937\n",
      "Epoch 2/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 293ms/step - accuracy: 0.6327 - loss: 0.6416\n",
      "Epoch 3/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 276ms/step - accuracy: 0.8736 - loss: 0.3184\n",
      "Epoch 4/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 277ms/step - accuracy: 0.9436 - loss: 0.2072\n",
      "Epoch 5/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 271ms/step - accuracy: 0.9617 - loss: 0.1234\n",
      "Epoch 6/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 272ms/step - accuracy: 0.9781 - loss: 0.0803\n",
      "Epoch 7/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 281ms/step - accuracy: 0.9777 - loss: 0.0670\n",
      "Epoch 8/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 272ms/step - accuracy: 0.9857 - loss: 0.0519\n",
      "Epoch 9/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 281ms/step - accuracy: 0.9947 - loss: 0.0218\n",
      "Epoch 10/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 291ms/step - accuracy: 0.9954 - loss: 0.0214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x170930e07f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LSTM \n",
    "# Each word is represented to LSTM as a vector of 32 elements and the LSTM\n",
    "# is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both\n",
    "# LSTM and the dense layer. Train the model using 10-50 epochs and batch\n",
    "# size of 10.\n",
    "\n",
    "# Switched to keras LSTM model because my pytorch LSTM model was not converging no matter what I tried.\n",
    "lstm_training_top_5000 = np.where(padded_training_seq > 5000, 0, padded_training_seq)\n",
    "training_labels = full_training_df['label']\n",
    "training_labels = training_labels.to_numpy()\n",
    "training_labels = (training_labels+1)//2\n",
    "\n",
    "# https://keras.io/api/layers/recurrent_layers/lstm/ \n",
    "# I did reference ChatGPT for the below to ensure I was applying every step correctly\n",
    "model_lstm = keras.Sequential()\n",
    "model_lstm.add(Embedding(5001, 32))\n",
    "model_lstm.add(LSTM(units=32, return_sequences= True, dropout= 0.2, recurrent_dropout= 0.2)) \n",
    "model_lstm.add(LSTM(units=32, return_sequences= True, dropout= 0.2, recurrent_dropout= 0.2)) \n",
    "model_lstm.add(LSTM(units=32, return_sequences= True, dropout= 0.2, recurrent_dropout= 0.2)) \n",
    "model_lstm.add(LSTM(units=32, return_sequences= False, dropout= 0.2, recurrent_dropout= 0.2)) # 4th stacked layer that takes outputs from three previous and outputs to the hidden layer shown in the project chart\n",
    "model_lstm.add(Dense(256))\n",
    "model_lstm.add(Activation('relu'))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.add(Activation('sigmoid'))\n",
    "\n",
    "model_lstm.compile(optimizer= 'adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model_lstm.fit(lstm_training_top_5000, training_labels, epochs= 10, batch_size= 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.6497 - loss: 1.7369\n",
      "Test accuracy: 0.6906354427337646\n"
     ]
    }
   ],
   "source": [
    "lstm_testing_top_5000 = np.where(padded_test_seq > 5000, 0, padded_test_seq)\n",
    "testing_labels = full_test_df['label']\n",
    "testing_labels = testing_labels.to_numpy()\n",
    "testing_labels = (testing_labels+1)//2\n",
    "\n",
    "loss, accuracy = model_lstm.evaluate(lstm_testing_top_5000,testing_labels)\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
